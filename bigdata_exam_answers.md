# ملخص أسئلة مقرر البيانات الضخمة (22 سؤال)

ملاحظة: الإجابات مبنية على ملفات المقرر في هذا المجلد. البنود التي كانت داخل صور في الـPDF تم استخراجها عبر OCR، وسيُشار لها بذلك.
المصادر الأساسية: `Lec 1 Lect 2 Lec 3.pdf`, `Lect  4 Big Data2 _011245.pdf`, `Big Data with Hadoop.pdf`.

## سؤال 1: ماهي البيانات الكبيرة (الضخمة)؟ مع ذكر خصائص البيانات الضخمة؟
**الشرح**: البيانات الضخمة (Big Data) هي مجموعات بيانات كبيرة ومعقدة جدًا لا تكفي الأدوات التقليدية للتعامل معها بكفاءة، وهدفها المعالجة والتخزين والتحليل لاستخراج رؤى ودعم القرار.
**الخصائص الأساسية**:
- الحجم (Volume)
- السرعة (Velocity)
- التنوع (Variety)
**مثال واقعي**: منصات التواصل مثل Facebook وTwitter وInstagram وLinkedIn كمصادر بيانات ضخمة.
**المصدر**: `Lect  4 Big Data2 _011245.pdf` صفحة 3 وصفحة 10.

## سؤال 2: لماذا نحتاج البيانات الضخمة؟ مع ذكر تطبيقات البيانات الضخمة؟
**الشرح**: تساعد البيانات الضخمة على زيادة دقة التحليل، ودعم القرارات السريعة، وتحقيق ميزة تنافسية.
**تطبيقات مذكورة في المقرر**:
- الرعاية الصحية (Healthcare Analytics)
- الخدمات المالية (Financial Services)
- التجارة الإلكترونية والبيع بالتجزئة (E-Commerce & Retail)
- الاتصالات (Telecommunications)
- الصناعة، المدن الذكية، الطاقة، التعليم، الإعلام، النقل، الموارد البشرية، الزراعة
**مثال واقعي**: شركات الخدمات المالية تستخدم تحليلات البيانات لكشف الاحتيال في المعاملات.
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحات 35-40.

## سؤال 3: ماهي قواعد البيانات مع ذكر المكونات الخاصة بقواعد البيانات؟
**الشرح**: قاعدة البيانات (Database) هي المكون المركزي الذي تُنظم فيه البيانات (جداول/صفوف/أعمدة/علاقات)، ويقوم DBMS بإدارة التخزين والاسترجاع والأمن والتزامن.
**مكونات بيئة DBMS**:
- قاعدة البيانات (Database)
- نظام إدارة قواعد البيانات (DBMS)
- العتاد (Hardware)
- البرمجيات (Software)
- المستخدمون (Users: DBA, Database Designer, Programmers/Analysts, End User)
**مثال واقعي**: MySQL، PostgreSQL، Oracle، SQL Server، MongoDB.
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحات 46-49.

## سؤال 4: دورة حياة قواعد البيانات، ودورة حياة البيانات الضخمة؟
**دورة حياة قواعد البيانات (RDBMS Life Cycle)** (تم استخراجها بالـOCR):
- Requirement Analysis (تحليل المتطلبات)
- Database Evaluation and Selection (تقييم واختيار قاعدة البيانات)
- Logical Database Design (التصميم المنطقي)
- Physical Database Design (التصميم الفيزيائي)
- Database Implementation and Data Loading (التنفيذ وتحميل البيانات)
- Testing and Performance Tuning (الاختبار وضبط الأداء)
- Operate and Maintain (التشغيل والصيانة)
- Growth and Change (النمو والتغيير)
**دورة حياة البيانات الضخمة (مختصر من المقرر)**:
- توليد البيانات (Big Data Generation)
- جمع البيانات ونقلها لمنصة التخزين (Data Collection)
- المعالجة المسبقة (Data Preprocessing)
- التحليلات (EDA, Model Building, Model Evaluation)
- النشر (Deployment)
- المراقبة والصيانة (Monitoring & Maintenance)
- التصور (Visualization)
**مثال واقعي**: استخدام Hadoop + HDFS + MapReduce ضمن دورة المعالجة.
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحة 45 (OCR)، و`Lect  4 Big Data2 _011245.pdf` صفحات 6-9 و18-25.

## سؤال 5: عرف نموذج البيانات وعدد أنواع نماذج البيانات المستخدمة؟
**الشرح**: نموذج البيانات (Data Model) تمثيل مفاهيمي يحدد كيفية هيكلة البيانات وتنظيمها وعلاقاتها داخل نظام قاعدة البيانات.
**الأنواع المذكورة**:
- النموذج العلائقي (Relational)
- نموذج الكيان–العلاقة (Entity-Relationship)
- النموذج الكائني (Object-Oriented)
- نموذج الرسم البياني (Graph)
- نموذج السلاسل الزمنية (Time-Series)
**مثال واقعي**: قواعد مثل Oracle/MySQL تعتمد النموذج العلائقي.
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحات 50-55.

## سؤال 6: مفاهيم المعاملات والنظام، حالات المعاملة، الخصائص المرغوبة للمعاملات؟
**الشرح**: المعاملة (Transaction) وحدة منطقية تتضمن عمليات قراءة/كتابة على قاعدة البيانات.
**حالات المعاملة**:
- نشطة (Active)
- التزام جزئي (Partially Committed)
- التزام (Committed)
- فشل (Failed)
- إنهاء (Terminated)
**خصائص المعاملة (ACID)**:
- الذرية (Atomicity)
- الاتساق (Consistency)
- العزل (Isolation)
- المتانة (Durability)
**مثال واقعي**: تحويل بنكي كامل يُعد معاملة واحدة.
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحات 63-73.

## سؤال 7: عيوب قاعدة البيانات العلائقية؟
**الشرح**:
- صعوبة التعامل مع البيانات الضخمة جدًا بكفاءة (Handling Big Data)
- اختناقات الأداء والحاجة لإعادة التصميم (Performance Bottlenecks)
- تكلفة عالية عند التوسع الرأسي (Scale Up)
- صعوبة التعامل مع البيانات شبه/غير المنظمة
- التعقيد في البيئات الموزعة (Distributed Environments)
**مثال واقعي**: الاتجاه إلى NoSQL مثل Cassandra أو MongoDB لمعالجة الحجم الكبير.
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحات 80-81.

## سؤال 8: الفرق بين قواعد البيانات العلائقية والبيانات الكبيرة (Attributes of Big Data vs RDBMS)
**الفروقات الأساسية**:
- الحجم: من GB/TB في RDBMS إلى PB/ZB في Big Data
- التنظيم: مركزي (Centralized) مقابل موزع (Distributed)
- نوع البيانات: منظمة فقط مقابل منظمة/شبه منظمة/غير منظمة
- العتاد: خوادم عالية مقابل أجهزة سلع (Commodity)
- التحديثات: قراءة/كتابة متكررة مقابل كتابة مرة وقراءة مرات
- المخطط: ثابت مقابل مرن/ديناميكي
**مثال واقعي**: التخزين الموزع في HDFS ضمن Hadoop.
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحة 81.

## سؤال 9: ماهي قواعد بيانات NoSQL؟ أنواعها؟ مميزاتها؟
**الشرح**: NoSQL قواعد بيانات غير علائقية (Non-Relational)، مصممة للتوسع الأفقي والتعامل مع البيانات الضخمة وشبه/غير المنظمة.
**الأنواع**:
- Key-Value
- Document-Oriented
- Graph
- Column-Oriented
**المميزات**:
- بدون مخطط (Schema-less)
- توسع أفقي (Horizontal Scaling)
- حوسبة موزعة (Distributed Computing)
- تكلفة أقل
- مناسبة لأحجام بيانات هائلة
**مثال واقعي**: MongoDB، HBase، Cassandra، Amazon DynamoDB، Google Bigtable.
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحات 82-105 و107.

## سؤال 10: نظرية CAP وخصائص BASE؟
**نظرية CAP**: لا يمكن لأي نظام موزع تحقيق الاتساق (Consistency) والتوفر (Availability) وتحمل الانقسام (Partition Tolerance) معًا في الوقت نفسه؛ عادة يتم تحقيق اثنين فقط.
**خصائص BASE**:
- متاحة بشكل أساسي (Basically Available)
- حالة ناعمة (Soft State)
- اتساق نهائي (Eventually Consistent)
**مثال واقعي**:
- AP: Amazon DynamoDB، Apache Cassandra
- CP: Google Cloud Spanner، PostgreSQL
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحات 84-94 و88.

## سؤال 11: مصادر البيانات الضخمة ودورة حياتها؟
**المصادر الشائعة**:
- وسائل التواصل الاجتماعي (Social Media)
- بيانات المعاملات (Transaction Data)
- بيانات الاستشعار/إنترنت الأشياء (Sensor Data/IoT)
- بيانات الويب والنقرات (Web & Clickstream)
**الدورة (مختصر)**: توليد البيانات، جمعها، معالجتها مسبقًا، التحليلات، التصور.
**مثال واقعي**: Facebook وTwitter وInstagram وLinkedIn كمصادر بيانات اجتماعية.
**المصدر**: `Lect  4 Big Data2 _011245.pdf` صفحات 10-11 و18.

## سؤال 12: عرف المعالجة المسبقة للبيانات وخطواتها بالتفصيل؟
**الشرح**: المعالجة المسبقة (Data Preprocessing) تحويل البيانات الأولية إلى بيانات متسقة ودقيقة مناسبة للتحليل.
**الخطوات الرئيسية**:
- تنظيف البيانات (Data Cleaning: Missing Values, Duplicates, Inaccuracies, Standardization, Outliers, Inconsistencies)
- تحويل البيانات (Data Transformation)
- تقليل البيانات (Data Reduction)
- التعامل مع البيانات غير المتوازنة (Imbalanced Data)
- التعامل مع بيانات السلاسل الزمنية (Time-Series Data)
- هندسة السمات (Feature Engineering)
**مثال واقعي**: تنظيف بيانات منشورات Facebook/Twitter قبل التحليل.
**المصدر**: `Lect  4 Big Data2 _011245.pdf` صفحات 18 و30-36 و32-33.

## سؤال 13: أنواع البيانات؟ وكم نسبة توليد كل نوع خلال السنوات الأخيرة؟
**الأنواع**:
- بيانات منظمة (Structured)
- بيانات غير منظمة (Unstructured)
- بيانات شبه منظمة (Semi-structured)
**النسبة المذكورة في المقرر**:
- حوالي 80% من البيانات التي يتم إنشاؤها هي غير منظمة.
**مثال واقعي**: الصور والمنشورات في Facebook تُعد بيانات غير منظمة.
**المصدر**: `Lect  4 Big Data2 _011245.pdf` صفحات 14-16.

## سؤال 14: ما هي Hadoop؟ مكوناتها؟ الفرق بين Hadoop 1 و Hadoop 2؟
**الشرح**: Hadoop إطار عمل مفتوح المصدر للتخزين الموزع ومعالجة البيانات الضخمة.
**المكونات الأساسية**: Hadoop + HDFS (Storage) + MapReduce (Processing).
**Hadoop 1.x (قيود مذكورة)**:
- يدعم فقط المعالجة الدُفعية (Batch Processing) عبر MapReduce
- لا يدعم المعالجة الفورية (Real-time Processing)
- NameNode واحد وJobTracker كنقطة فشل
- لا يدعم التوسع الأفقي بشكل كافٍ
**Hadoop 2.x (من OCR)**:
- منظومة تحتوي على YARN مع Resource Manager وNode Manager
- مذكور ضمن "Hadoop 2.x Core Components" مع عناصر HDFS مثل NameNode
**مثال واقعي**: تشغيل مهام MapReduce على بيانات مخزنة في HDFS ضمن Hadoop.
**المصدر**: `Big Data with Hadoop.pdf` صفحة 31، وصفحات 36-38 (OCR)، و`Lect  4 Big Data2 _011245.pdf` صفحة 6.

## سؤال 15: ما هو Hadoop Ecosystem؟ اذكر مكونات Hadoop؟
**Hadoop Ecosystem (Hadoop 1.0) حسب OCR**:
- HDFS
- MapReduce
- HBase
- Oozie
- Flume
- Sqoop
**Hadoop Ecosystem (Hadoop 2.0) حسب OCR**:
- YARN (Resource Manager, Application Manager)
- HBase أعلى HDFS
**مثال واقعي**: Oozie لجدولة سير العمل، وHive كمستودع بيانات.
**المصدر**: `Big Data with Hadoop.pdf` صفحات 30 و34 (OCR) + `Big Data with Hadoop.pdf` صفحات 32-34.

## سؤال 16: اشرح أداة Mahout وعدد جميع مكوناتها؟
**الشرح**: Apache Mahout إطار جبر خطي موزع مع Scala DSL يساعد علماء البيانات والإحصائيين على تنفيذ خوارزمياتهم بسرعة.
**معلومة عامة خارج المحاضرات (غير واردة نصيًا في المقرر)**:
- وحدات/مجالات شائعة في Mahout: التوصيات (Recommendation/Collaborative Filtering)، التصنيف (Classification)، التجميع (Clustering)، تخفيض الأبعاد (Dimensionality Reduction)، مكتبة الجبر الخطي (Mahout Math).
**مثال واقعي**: Mahout كأداة تعلم آلي موزعة ضمن Hadoop.
**المصدر**: `Big Data with Hadoop.pdf` صفحة 33.

## سؤال 17: عدد التحديات التي تواجه البيانات الضخمة؟
**تحديات جودة البيانات المذكورة في المقرر**:
- أخطاء في البيانات (Inaccuracies)
- عدم اكتمال البيانات (Missing Values)
- عدم اتساق البيانات (Inconsistencies)
- التكرارات (Duplicates)
- القيم المتطرفة (Outliers)
- اختلاف تنسيقات الإدخال والمصادر غير المتجانسة
**ملاحظة**: لم يرد نص صريح عن تحديات الحجم/السرعة/التنوع، لكن المقرر يشير لتحديات جمع ومعالجة البيانات من مصادر متعددة.
**مثال واقعي**: بيانات وسائل التواصل تحتاج تنظيفًا بسبب التكرار وعدم الاتساق.
**المصدر**: `Lect  4 Big Data2 _011245.pdf` صفحات 30-36.

## سؤال 18: ما هي أفضل سحابة يمكن استخدامها في معالجة البيانات الضخمة؟
**الشرح**: المقرر لا يحدد "الأفضل"، لكنه يذكر منصات سحابية (Cloud Platforms) لمعالجة البيانات الضخمة.
**منصات مذكورة في المقرر**: Amazon Web Services (AWS)، Microsoft Azure، Google Cloud Platform.
**مثال واقعي**: استخدام AWS أو Azure أو GCP لمعالجة البيانات الضخمة حسب الحاجة.
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحة 16.

## سؤال 19: ما المقصود بـMachine Learning التعلّم الآلي؟ وما الأنواع الرئيسية؟
**الشرح في المقرر**: ذُكر التعلم الآلي ضمن تحليلات البيانات الضخمة لكنه لم يُعرَّف تفصيليًا في النص المستخرج.
**معلومة عامة خارج المحاضرات (ليست من المقرر)**:
- التعلّم المراقَب (Supervised Learning): يتعلم من بيانات لها مُخرجات معروفة. مثال: تصنيف البريد المزعج.
- التعلّم غير المراقَب (Unsupervised Learning): يكتشف الأنماط دون مُخرجات. مثال: تجميع العملاء حسب السلوك.
- التعلّم بالتعزيز (Reinforcement Learning): يتعلم عبر المكافآت والعقوبات. مثال: تدريب وكيل للعب لعبة.
**مثال واقعي من المقرر**: تحليل النصوص غير المنظمة أصبح ممكنًا بفضل التعلم الآلي.
**المصدر**: `Lect  4 Big Data2 _011245.pdf` صفحة 7.

## سؤال 20: الفرق بين HDFS و YARN؟
**HDFS**: نظام ملفات موزع (Distributed File System) لتخزين واسترجاع البيانات عبر عقد متعددة.
**YARN**: مسؤول عن إدارة الموارد وجدولة/مراقبة الوظائف عبر مدير موارد (Resource Manager) ومدير تطبيق (Application Manager).
**مثال واقعي**: تشغيل مهام MapReduce على بيانات في HDFS عبر YARN.
**المصدر**: `Big Data with Hadoop.pdf` صفحات 18 و34.

## سؤال 21: تطبيقات أداة Mahout؟
**الشرح (من OCR)**: محرك توصيات باستخدام Mahout.
**مثال واقعي**: توصيات LinkedIn (LinkedIn Recommendations) باستخدام Mahout.
**المصدر**: `Big Data with Hadoop.pdf` صفحة 35 (OCR).

## سؤال 22: الأنظمة الموزعة وخصائصها والتحديات؟
**خصائص مذكورة**:
- توزيع البيانات عبر أكثر من جهاز لزيادة الموثوقية
- المعالجة المتوازية (Parallel Processing)
**تحديات/مقايضات رئيسية (CAP)**: صعوبة الجمع بين الاتساق والتوفر وتحمل الانقسام في نظام موزع.
**مثال واقعي**: DynamoDB/Cassandra (AP) مقابل Google Cloud Spanner/PostgreSQL (CP).
**المصدر**: `Lec 1 Lect 2 Lec 3.pdf` صفحات 84-87 و88 و102.
